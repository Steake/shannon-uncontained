# Shannon Uncontained - Environment Variables
# Copy this file to .env and fill in the values

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================
# Shannon supports multiple LLM providers. Configure ONE of the following:

# -----------------------------------------------------------------------------
# CLOUD PROVIDERS (require API keys)
# -----------------------------------------------------------------------------

# Option 1: GitHub Models (Recommended for free tier)
# Get your token from: https://github.com/settings/tokens
GITHUB_TOKEN=

# Option 2: OpenAI API
# Get your key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=

# Option 3: Anthropic Claude (requires Claude Code or separate SDK)
# Get your key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=

# -----------------------------------------------------------------------------
# LOCAL PROVIDERS (no API key needed)
# -----------------------------------------------------------------------------

# Option 4: Ollama (local)
# Install: https://ollama.ai
# Run: ollama serve
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2

# Option 5: llama.cpp (local)
# Run: python -m llama_cpp.server --model your_model.gguf
# LLM_PROVIDER=llamacpp
# LLM_MODEL=local-model

# Option 6: LM Studio (local)
# Download: https://lmstudio.ai
# Start local server from UI
# LLM_PROVIDER=lmstudio
# LLM_MODEL=local-model

# -----------------------------------------------------------------------------
# CUSTOM ENDPOINT (any OpenAI-compatible API)
# -----------------------------------------------------------------------------

# Option 7: Custom endpoint
# LLM_PROVIDER=custom
# LLM_BASE_URL=https://your-endpoint.com/v1
# LLM_MODEL=your-model-name

# =============================================================================
# PROVIDER SELECTION (Optional)
# =============================================================================
# If multiple keys are set, specify which provider to use:
# Options: github, openai, ollama, llamacpp, lmstudio, custom
# Default: auto-detect based on which key is set
# LLM_PROVIDER=github

# =============================================================================
# MODEL SELECTION (Optional)
# =============================================================================
# Override the default model for your provider:
#
# GitHub Models: openai/gpt-4.1, openai/gpt-4o, meta-llama-3.1-405b-instruct
# OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229
# Ollama: llama3.2, codellama, mistral, mixtral
# llama.cpp / LM Studio: depends on your loaded model
#
# LLM_MODEL=

# =============================================================================
# CUSTOM ENDPOINT OVERRIDE (Optional)
# =============================================================================
# Override the base URL for any provider (useful for proxies, VPNs, etc.)
# LLM_BASE_URL=

# =============================================================================
# OPTIONAL SETTINGS
# =============================================================================
# Max output tokens (for long reports)
# CLAUDE_CODE_MAX_OUTPUT_TOKENS=64000
